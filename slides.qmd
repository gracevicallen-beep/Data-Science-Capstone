---
title: "Generalized Additive Models in Fraud Detection"
subtitle: "Data Science Capstone Project"
author: "Grace Allen, Kesi Allen, Sonya Melton, Pingping Zhou"
date: "December 10, 2025"
format:
  revealjs:
    code-fold: true
    self-contained: true
theme: reveal.scss
output-dir: report1      # Folder where rendered HTML will go
bibliography: references.bib
#always_allow_html: true # this allows to get PDF with HTML features
csl: apa.csl 
execute: 
  warning: false
  message: false
course: "Capstone Projects in Data Science"
editor:
  markdown: 
    wrap: 72
---

# Introduction

## **What are generalized additive models?**

-   Not your typical straight-line regression — GAMs let patterns curve
    naturally

-   Great at uncovering hidden trends in messy real-world data

-   Each feature gets its own shape, showing where risk rises or falls

-   Makes the model’s behavior easy to explain to non-technical teams

-   Perfect for fraud detection, where small pattern changes matter

## Brief History of GAMs

Generalized Additive Models were introduced in the late 1980s as a way
to add flexibility to traditional regression models. @Hastie_1986
developed the framework to allow each predictor in a model to follow its
own smooth pattern rather than forcing everything into a straight line.
Through the 1990s and early 2000s, the approach grew in popularity in
fields that needed interpretable models, including public health,
ecology, and social sciences.

## Brief History of GAMs

A major step forward came with the development of the mgcv package in R,
created by Simon Wood [@Wood_2017]. His work added modern smoothing
techniques, automatic penalty selection, and faster computation, making
GAMs practical for large and noisy datasets [@Wood_2025]. Today, GAMs
are widely used in finance, fraud detection, risk scoring, and other
areas where organizations need both predictive accuracy and clear
explanations [@DalPozzolo_2014; @DGAM_2021; @GamHP_2020].

## GAMS in Action: Real World Uses + Our Study

**GAMs help uncover nonlinear relationships and subtle patterns across
diverse domains:**

-   Financial Analytics: Detecting anomalies and potential fraud in
    transaction data [@Brossart_2015]

-   Banking & Insurance: Modeling risk scores in banking and insurance
    [@Tragouda_2024]

## GAMS in Action: Real World Uses + Our Study

**GAMs help uncover nonlinear relationships and subtle patterns across
diverse domains:**

-   Environmental Science: Forecasting trends in environmental and
    climate research [@Guisan_2002; @Detmer_2025]

-   Public Health: Understanding health outcomes and public health
    patterns [@White_2020]

## **Our Project: Study Context: GAMs for Fraud Detection**

-   Toolset: RStudio + package

-   Dataset: Kaggle’s Fraud Detection Transactions [@Ashar_2024]

-   Purpose: Identify predictive variables linked to fraudulent activity

-   Context: Synthetic but realistic data for controlled testing

***Here’s how we used GAMs to explore patterns in the fraud dataset.***

# Methods

## **GAM Modeling Overview**

-   GAMs extend traditional regression [@Hastie_1986]

-   Capture nonlinear predictor-response relationships

-   Use spline-based smooth functions

-   Combine continuous + categorical predictors

-   Fit with mgcv (penalized splines + GCV)

-   Model outputs interpretable smooth effects

-   Goal: Estimate probability of fraud

## Modeling Workflow Steps

1.  Acquire synthetic Kaggle fraud dataset (50k rows, 21 features)
2.  Explore distributions and identify skew/outliers
3.  Clean data and review categorical variables
4.  Summarize transaction type, device type, and merchant category
5.  Visualize patterns using histograms and nonlinear trend checks

## Modeling Workflow Steps

1.  Check assumptions with k-index, QQ plot, and residual diagnostics
2.  Fit GAM and interpret numeric and categorical smooth effects
3.  Evaluate performance using confusion matrix, ROC curve, and AUC
    (0.73)
4.  Summarize findings and finalize interpretation

## GAM Equation

$$ g(\mu) = \alpha + s_1(X_1) + s_2(X_2) + \dots + s_p(X_p) $$

-   \(g\) = link function (logit for binary fraud) [@HalDa_2012]

-   Smooth functions capture nonlinear effects

-   Additive contributions from each predictor

-   Balances flexibility + interpretability [@Hastie_Tibshirani_1990]

## GAM Assumptions (Fraud Context)

-   Logit link approximates fraud probability

-   Additive and independent predictor effects

-   Smooth, gradual functional relationships

-   Binomial response distribution

-   Independent observations [@Wood_2017]

-   Low predictor multicollinearity

-   Penalization prevents overfitting [@Wood_2017]

## Why We Chose GAMs For Fraud Detection

-   Captures nonlinear fraud patterns

-   Handles rare, imbalanced outcomes

-   Produces interpretable smooth risk curves

-   Supports regulatory transparency

-   Balances accuracy + interpretability

-   Strong literature support for fraud analytics

-   Scalable through mgcv’s automated smoothing [@Wood_2017]

## Practical Advantages & Relevance to Real-World Analytics

-   Supports investigative decision-making

-   Shows monotonic or nonlinear risk curves

-   Supports investigative decision-making

-   Can benchmark or surrogate black-box models

-   High recall for suspicious transactions

-   Useful for auditors, fraud teams, analysts

-   Aligns with both operational and compliance needs

# Analysis and Results

# Data Exploration and Visualization

## **Dataset Description**

***What It Is***

-   A synthetic dataset built to mimic real financial transactions

-   Privacy‑safe: no real people’s data used

-   Hosted on Kaggle

## **Dataset Description**

***Why We Use It***

-   Train fraud detection models for binary classification tasks

-   Spot fraud: each transaction labeled as fraud (1) or not fraud (0)

## **Dataset Description**

***What Makes It Special***

Realistic fraud patterns:

-   Groups of fraudulent transactions

-   Subtle, hard‑to‑notice anomalies

-   Odd user behaviors

-   Large & diverse records: balances normal vs. rare fraud cases →
    addresses class imbalance.

## **Dataset Key Characteristics**

***What's Inside***

-   50,000 Rows: A good amount of data to work with.

-   Two Labels: Every transaction is marked as either: 1 = Fraud 0 = Not
    Fraud

## Data Features

**21 features across three categories:**

-   Numbers: Like transaction amounts, risk scores, account balances.

-   Categories: Transaction types (payment, transfer, withdrawal),
    device types, merchant categories.

-   Time Data: When transactions happened (time, day) and their
    sequence.

## **Label Distribution Class Imbalance**

-   Fraudulent transactions are a small percentage, reflecting
    real-world scenarios.

-   Behavioral Realism: Includes unusual spending, behavioral signals,
    and high-risk profiles.

-   Modeling flexibility: supports interpretable (GAMs, logistic
    regression) or high-performance (XGBoost) approaches

## Dataset Visualizations

**Categorical Distribution**

```{r}
# Load libraries
library(tidyverse)
library(janitor)
library(gt)
library(scales)

# === Load dataset ===
data_path <- "synthetic_fraud_dataset.csv"
df <- readr::read_csv(data_path, show_col_types = FALSE) |>
  clean_names()

# === Create count tables ===
tbl_type <- df |>
  count(transaction_type, name = "Count") |>
  arrange(desc(Count)) |>
  rename(Type = transaction_type)

tbl_device <- df |>
  count(device_type, name = "Count") |>
  arrange(desc(Count)) |>
  rename(Device = device_type)

tbl_merchant <- df |>
  count(merchant_category, name = "Count") |>
  arrange(desc(Count)) |>
  rename(Merchant_Category = merchant_category)

# === Blue Theme for gt Tables ===
style_blue_gt <- function(.data, title_text) {
  .data |>
    gt() |>
    tab_header(title = md(title_text)) |>
    fmt_number(columns = "Count", decimals = 0, sep_mark = ",") |>
    tab_options(
      table.font.names = "Arial",
      table.font.size  = 14,
      data_row.padding = px(6),
      heading.align    = "left",
      table.border.top.color    = "darkblue",
      table.border.top.width    = px(3),
      table.border.bottom.color = "darkblue",
      table.border.bottom.width = px(3)
    ) |>
    tab_style(
      style = list(cell_fill(color = "darkblue"),
                   cell_text(color = "white", weight = "bold")),
      locations = cells_title(groups = "title")
    ) |>
    tab_style(
      style = list(cell_fill(color = "steelblue"),
                   cell_text(color = "white", weight = "bold")),
      locations = cells_column_labels(everything())
    ) |>
    opt_row_striping() |>
    cols_align("right", columns = "Count")
}

# === Render all three blue tables ===
style_blue_gt(tbl_type, "Table 1 – Transaction Types and Counts")
style_blue_gt(tbl_device, "Table 2 – Device Types and Counts")
style_blue_gt(tbl_merchant, "Table 3 – Merchant Categories and Counts")

```

## Distribution of Variables

```{r}
library(tidyverse)
library(lubridate)
library(patchwork)  # for arranging multiple ggplots

# Load dataset
fraud_data <- read.csv("synthetic_fraud_dataset.csv")

# Convert Timestamp to date and calculate Issuance_Year if needed
fraud_data <- fraud_data %>%
  mutate(
    Timestamp = ymd_hms(Timestamp, quiet = TRUE),  # adjust format if needed
    Transaction_Year = year(Timestamp),
    Issuance_Year = Transaction_Year - Card_Age
  ) %>%
  filter(!is.na(Card_Age))  # remove rows with NA in Card_Age

# Variables to plot (move Transaction_Amount to last)
numeric_vars <- c("Account_Balance", "Transaction_Distance", "Risk_Score", "Card_Age", "Transaction_Amount")

# Create a list to store plots
plot_list <- list()

# Generate plots and store in the list
for (var in numeric_vars) {
  p <- ggplot(fraud_data, aes_string(x = var)) +
    geom_histogram(fill = "steelblue", color = "white", bins = 30) +
    labs(title = paste("Distribution of", var),
         x = var,
         y = "Count") +
    theme_light()
  
  plot_list[[var]] <- p
}

# Arrange plots in a grid: 2 plots per row
(plot_list[[1]] | plot_list[[2]]) /
(plot_list[[3]] | plot_list[[4]]) /
plot_list[[5]]  # Transaction_Amount appears last
```

## Card Age

```{r}
library(tidyverse)
library(lubridate)
# Load libraries
library(ggplot2)
library(dplyr)
library(tidyr)    # For pivot_longer
library(gridExtra) # For arranging plots
#install.packages("moments") 
library(moments)   # For skewness and kurtosis
# Load dataset
fraud_data <- read.csv("synthetic_fraud_dataset.csv")

# Convert Timestamp to date, calculate Transaction Year and Issuance Year, exclude NAs
fraud_data <- fraud_data %>%
  mutate(
    Timestamp = ymd_hms(Timestamp),               # adjust if format differs
    Transaction_Year = year(Timestamp),
    Issuance_Year = Transaction_Year - Card_Age
  ) %>%
  filter(!is.na(Issuance_Year), !is.na(Card_Age))  # remove rows with NA

# Bin Issuance Year into 5-year ranges and drop unused NA factor levels
fraud_data <- fraud_data %>%
  mutate(
    Issuance_Year_Bin = cut(Issuance_Year,
                             breaks = seq(2000, 2025, by = 5),
                             right = FALSE,
                             labels = c("2000-2004","2005-2009","2010-2014","2015-2019","2020-2024"))
  ) %>%
  filter(!is.na(Issuance_Year_Bin))  # drop any rows that fall outside the bins

# Histogram
ggplot(fraud_data, aes(x = Issuance_Year_Bin)) +
  geom_bar(fill = "steelblue", color = "white") +
  labs(title = "Figure 2. Card Age by Issuance Year Range",
       x = "Card Issuance Year Range",
       y = "Count") +
  theme_light()
```

## Non-linearity Check

```{r}
library(tidyverse)
# Load dataset
fraud_data <- read.csv("synthetic_fraud_dataset.csv")
# Ensure Fraud_Label is numeric (0/1)
fraud_data <- fraud_data %>%
  mutate(Fraud_Label = as.numeric(Fraud_Label))

# Nonlinearity check: Transaction Amount vs Fraud Probability
ggplot(fraud_data, aes(x = Transaction_Amount, y = Fraud_Label)) +
  geom_smooth(method = "loess", se = FALSE, color = "darkblue") +
  labs(title = "Figure 3. Transaction Amount and Fraud Probability",
       x = "Transaction Amount",
       y = "Fraud Probability") +
  theme_light()
```

# Modeling and Results

## **Assumptions**

![](images/rvf.png){fig-align="center"}

## GAM Analysis for Numeric Variables

```{r}
#install.packages("caret")
library(mgcv)
library(dplyr)
library(caret)
# Load your data
data <- read.csv("synthetic_fraud_dataset.csv")
# Build GAM model with Risk_Score included as a smooth term
gam_model <- gam(Fraud_Label ~
               	Merchant_Category +
               	Is_Weekend +
               	s(Transaction_Amount) +
               	s(Account_Balance) +
               	s(Card_Age) +
               	s(Risk_Score),
             	family = binomial(link = "logit"),
             	data = data)
par(mfrow = c(2, 2), mar = c(4, 4, 3, 2))
plot(gam_model, select = 1, shade = TRUE, col = "blue", lwd = 2,
 	shade.col = "lightblue", main = "s(Transaction_Amount)")
plot(gam_model, select = 2, shade = TRUE, col = "green4", lwd = 2,
 	shade.col = "lightgreen", main = "s(Account_Balance)")
plot(gam_model, select = 3, shade = TRUE, col = "purple", lwd = 2,
 	shade.col = "plum", main = "s(Card_Age)")
plot(gam_model, select = 4, shade = TRUE, col = "red", lwd = 2,
 	shade.col = "pink", main = "s(Risk_Score)")
par(mfrow = c(1, 1))
```

## GAM Analysis for Categorical Variables

**Transaction_TypeBankTransfer**\
p.value: 0.498\
OR_low: 0.931\
OR_high: 1.035

**Transaction_TypeOnline**\
p.value: 0.546\
OR_low: 0.933\
OR_high: 1.037

## GAM Analysis for Categorical Variables

**Merchant_CategoryElectronics**\
p.value: 0.734\
OR_low: 0.952\
OR_high: 1.072

**Merchant_CategoryGroceries**\
p.value: 0.531\
OR_low: 0.960\
OR_high: 1.082

## GAM Model for Key Predictor

```{r message=FALSE, warning=FALSE}
# Load packages
library(mgcv)
library(ggplot2)
library(dplyr)
library(broom)

fraud_data <- read.csv("synthetic_fraud_dataset.csv")

# Make sure Fraud_Label is numeric (0 = legit, 1 = fraud)
fraud_data <- fraud_data %>%
  mutate(Fraud_Label = as.numeric(Fraud_Label))

# Fit the Generalized Additive Model (GAM)
risk_gam <- gam(Fraud_Label ~ s(Risk_Score),
                data = fraud_data,
                family = binomial(link = "logit"))

# Tidy model summary (clean output)
smooth_summary <- tidy(risk_gam, parametric = FALSE)


# Predicted probabilities
fraud_data <- fraud_data %>%
  mutate(predicted_prob = predict(risk_gam, type = "response"))

# Visualization: Predicted probability by Risk Score with descriptive legend
ggplot(fraud_data, aes(x = Risk_Score, y = predicted_prob)) +
  geom_point(aes(color = "Raw Data"), alpha = 0.3) +   # raw data points
  geom_smooth(aes(color = "Fitted GAM Curve"), se = TRUE, linewidth = 1) +  # GAM fit
  scale_color_manual(
    name = "Legend",
    values = c(
      "Raw Data" = "steelblue",
      "Fitted GAM Curve" = "red"
    )
  ) +
  labs(
    title = "Figure 10. Fraud Probability vs. Risk Score",
    x = "Risk Score",
    y = "Predicted Probability of Fraud"
  ) +
  theme_light(base_size = 13) +
  theme(plot.title = element_text(face = "bold", hjust = 0.5))
```

## GAM Equation for Key Predictor

GAM equation structure:

$$ g(\mu) = \alpha + s_1(X_1) + s_2(X_2) + \dots + s_p(X_p) $$

our model simplifies to a single predictor:

$$ \text{logit}(\Pr(\text{Fraud} = 1)) = \alpha + s(\text{Risk_Score}) $$

where alpha = 1.9109 is the intercept, representing the baseline
log-odds of fraud when Risk_Score is zero.

## Model Diagnostics and Performance Metrics

```{r}
##Confusion Matrix
##Install once: 
#install.packages(
#  c("mgcv", "pROC", "caret", "dplyr", "ggplot2", "scales"),
  repos = "https://cloud.r-project.org"
#)

library(mgcv)
library(pROC)
library(caret)
library(dplyr)
library(ggplot2)
library(scales)

data <- read.csv("synthetic_fraud_dataset.csv", stringsAsFactors = FALSE)
# 2. Data Preprocessing
# ------------------------------------------------

# Convert the target variable and categorical predictors to factors
data$Fraud_Label <- factor(data$Fraud_Label, levels = c(0, 1))
data$Is_Weekend <- factor(data$Is_Weekend)
data$Previous_Fraudulent_Activity <- factor(data$Previous_Fraudulent_Activity)
data$Device_Type <- factor(data$Device_Type)
data$Card_Type <- factor(data$Card_Type)

# ------------------------------------------------
# 3. Data Splitting (70% Train, 30% Test)
# ------------------------------------------------

set.seed(42) # For reproducibility
train_index <- createDataPartition(data$Fraud_Label, p = 0.7, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]

# ------------------------------------------------
# 4. GAM Fitting (Logistic Model)
# ------------------------------------------------

# Use smooth terms (s()) for continuous variables to capture non-linear fraud patterns.
gam_model <- gam(
  Fraud_Label ~ s(Transaction_Amount) +
    s(Account_Balance) +
    s(Risk_Score) +
    s(Transaction_Distance) +
    Avg_Transaction_Amount_7d +
    Daily_Transaction_Count +
    Card_Age +
    Is_Weekend +
    Previous_Fraudulent_Activity +
    Device_Type +
    Card_Type,
  data = train_data,
  family = binomial(link = "logit"), # Logistic GAM for binary classification
  method = "REML"
)


# ------------------------------------------------
# 5. Prediction and AUC Calculation
# ------------------------------------------------

test_probabilities <- predict(gam_model, newdata = test_data, type = "response")

# Generate the ROC curve
roc_obj <- roc(test_data$Fraud_Label, test_probabilities)
auc_value <- auc(roc_obj)


# ------------------------------------------------
# 6. Confusion Matrix and Balanced Accuracy
# ------------------------------------------------

# Convert probabilities to classes (using 0.5 threshold)
predicted_classes <- factor(ifelse(test_probabilities > 0.5, 1, 0), levels = c(0, 1))
cm <- confusionMatrix(predicted_classes, test_data$Fraud_Label, positive = "1")
balanced_accuracy <- cm$byClass["Balanced Accuracy"]

# Prepare data for plotting
cm_table <- as.data.frame(cm$table)
names(cm_table) <- c("Pred", "Ref", "Freq")

cm_table <- cm_table %>%
  group_by(Ref) %>%
  mutate(Pct = Freq/sum(Freq)*100, Label = paste0(Freq, "\n(", round(Pct,1), "%)"))

# Create the heatmap plot
p_cm <- ggplot(cm_table, aes(x = Ref, y = Pred, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Label), color = "white", size = 6, fontface = "bold") +
  scale_fill_gradient(low = "#2c7bb6", high = "#d7191c") +
  labs(title = "Figure 11. Confusion Matrix", x = "Actual (Reference)", y = "Predicted") +
  theme_minimal() +
  coord_fixed()
print(p_cm)
```

## Model Diagnostics and Performance Metrics

```{r}
# Install once: 
#install.packages(c("mgcv","pROC","caret","dplyr","ggplot2","scales"))

library(mgcv)
library(pROC)
library(caret)
library(dplyr)
library(ggplot2)
library(scales)

## ROC Curve 

# Load the dataset
df <- read.csv("synthetic_fraud_dataset.csv", stringsAsFactors = FALSE)

# ------------------------------------------------
# 2. Data Preprocessing and Splitting
# ------------------------------------------------
df <- df %>%
  mutate(
    across(c(Transaction_Type, Device_Type, Location, Merchant_Category,
             Card_Type, Authentication_Method, IP_Address_Flag,
             Previous_Fraudulent_Activity, Is_Weekend), factor),
    Fraud_Label = factor(Fraud_Label, levels = c(0,1))
  )

set.seed(123)
train_idx <- createDataPartition(df$Fraud_Label, p = .70, list = FALSE)
train <- df[train_idx, ]
test  <- df[-train_idx, ]

# ------------------------------------------------
# 3. Fit Simple GAM (Focusing on Key Predictors)
# ------------------------------------------------
gam_mod <- gam(
  Fraud_Label ~
    s(Risk_Score, k = 10) +                  
    s(Transaction_Amount, k = 10) +
    s(Transaction_Distance, k = 10) +
    Previous_Fraudulent_Activity +
    Device_Type +
    Card_Type +
    Is_Weekend,
  family = binomial, 
  data = train,
  method = "REML",
  select = TRUE       
)

# ------------------------------------------------
# 4. Prediction and ROC/AUC Calculation
# ------------------------------------------------
test_prob <- predict(gam_mod, test, type = "response")
roc_obj <- roc(test$Fraud_Label, test_prob)
auc_val <- auc(roc_obj)

# ------------------------------------------------
# 5. ROC Curve Generation (Should now save to your setwd() folder)
# ------------------------------------------------

# Prepare data for ggplot2 plotting
roc_df <- data.frame(fpr = 1 - roc_obj$specificities, tpr = roc_obj$sensitivities)

# Create the ggplot2 visualization
p_roc <- ggplot(roc_df, aes(x = fpr, y = tpr)) +
  geom_ribbon(aes(ymin = 0, ymax = tpr), fill = "#2c7bb6", alpha = 0.2) +
  geom_line(color = "#2c7bb6", linewidth = 1.5) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray") +
  labs(title = "Figure 12. ROC Curve", 
       subtitle = paste("GAM Model AUC =", round(auc_val, 4)),
       x = "False Positive Rate (1 - Specificity)", 
       y = "True Positive Rate (Sensitivity)") +
  theme_minimal(base_size = 14) +
  coord_fixed() +
  scale_x_continuous(labels = scales::percent, breaks = seq(0,1,0.2)) +
  scale_y_continuous(labels = scales::percent, breaks = seq(0,1,0.2))

print(p_roc)
```

# Conclusion

## **Key Findings & Model Performance**

*What We Found:*

-   Risk Score: Strongest predictor — fraud probability spikes near 0.75

-   Transaction Amount: Moderate effect — larger transactions more
    likely to be fraudulent

## Model Performance

-   True Positives: 2,318

-   True Negatives: 10,102

-   False Positives: 77

-   False Negatives: 2,502

-   AUC (ROC Curve): 0.73 — good discriminative ability

## Limitations

-   Synthetic dataset limits real-world generalizability

-   Class imbalance affects recall for rare fraud case

-   More complex models may improve accuracy

## Insights & Next Steps

-   Combine GAMs with other ML methods

-   Test on real-time or streaming data

-   Refine thresholds for cost-sensitive decisions

# QUESTIONS?

## References
