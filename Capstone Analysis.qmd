---
title: "GAM Fraud Analysis"
subtitle: "Data Science Capstone Project"
author: "Grace Allen, Kesi Allen, Sonya Melton, Pingping Zhou"
date: "`r Sys.Date()`"
format: 
  html:
    code-fold: true       # Allows folding of code chunks
    self-contained: true  # Packages images and CSS in the HTML
output-dir: report1      # Folder where rendered HTML will go
bibliography: references.bib
csl: apa.csl 
execute: 
  warning: false
  message: false
css: styles.css 
course: "Capstone Projects in Data Science"
editor:
  markdown: 
    wrap: 72
---

## Slides

Slides: [slides.html](slides.html){target="_blank"} ( our slides html
link will go here)

## Introduction

Grace intro:

Generalized Additive Models (GAMs) build on traditional regression
methods by allowing more flexible and nonlinear relationships between
predictors and outcomes. Early work introduced GAMs as an extension of
generalized linear models that can capture complex patterns in data that
linear models often miss [@Hastie_1986]. Later research focused on
improving how GAMs are fit, making them more efficient and able to
handle larger datasets [@Detmer_2025]. These models have been widely
applied in different fields, such as ecology and biostatistics, where
they have been used to study species distributions [@Guisan_2002],
detect ecological thresholds [@Detmer_2025], and explore health outcomes
like alcohol consumption [@White_2020]. Together, these studies show
that GAMs are useful for uncovering hidden structure in data, but they
also highlight challenges such as the risk of overfitting, difficulty of
interpretation, and reliance on data quality.

More recent work has provided additional perspective on how GAMs operate
and when they are most reliable. A Bayesian approach has helped explain
how smoothing and variable selection can be understood as adding
structure to the model, while also giving clearer ways to measure
uncertainty [@Miller_2025]. At the same time, studies show that GAMs
perform best when data are rich and patterns are strong, while noisy or
sparse datasets can lead to misleading results [@Detmer_2025;
@Guisan_2002]. This makes GAMs well-suited as exploratory models that
highlight important nonlinear relationships, but less suited for drawing
final conclusions on their own. For our project on fraud detection,
these insights are valuable because they justify using GAMs to reveal
complex patterns in financial data, while reminding us to be careful
about issues like overfitting, noise, and interpretability.

Building on this foundation, our project applies GAMs to the Fraud
Detection Transactions Dataset from Kaggle [@Ashar_2024], a synthetic
but realistic dataset designed to simulate patterns of financial fraud.
Using RStudio and the mgcv package, we will conduct a series of GAM
analyses to identify which variables within the dataset serve as
significant indicators of fraud. This approach allows us to explore GAM
methodology in practice, evaluating both model performance and
interpretability while gaining hands-on experience with smoothing,
variable selection, and visualization techniques in a real-world-style
dataset.

Pingping intro:

Generalized Additive Models (GAMs) extend traditional regression by
modeling nonlinear relationships between predictors and outcomes,
offering greater flexibility than generalized linear models
[@Hastie_1986]. By capturing complex patterns, GAMs are widely applied
in fields such as ecology and biostatistics to model species
distributions [@Guisan_2002], detect ecological thresholds
[@Detmer_2025], and analyze health outcomes like alcohol consumption
[@White_2020]. Recent advancements have improved computational
efficiency for large datasets [@Wood_2025] and introduced Bayesian
approaches to enhance smoothing, variable selection, and uncertainty
quantification [@Miller_2025]. Despite these strengths, GAMs face
challenges, including overfitting, interpretability issues, and
sensitivity to noisy or sparse data, performing best when datasets are
robust and patterns are clear ([@Detmer_2025; @Guisan_2002], making them
powerful exploratory tools rather than definitive predictive models.

In this project, we apply GAMs to the Fraud Detection Transactions
Dataset [@Ashar_2024], a synthetic yet realistic Kaggle dataset
simulating financial fraud patterns. The dataset includes numerical,
categorical, and temporal features with a binary fraud label, providing
an ideal platform for exploring smoothing techniques, variable
selection, and model interpretability using RStudio and the **mgcv**
package. By analyzing these features, we aim to identify key fraud
indicators while addressing challenges such as overfitting, noise, and
class imbalance, demonstrating the practical application of GAMs in
financial fraud detection.

### Literature Review

Pingping:

1.Tragouda et al. (2024) highlight GAMsâ€™ ability to balance
interpretability and predictive performance in fraud detection, such as
bank cheque fraud. While challenges like uneven data and changing fraud
patterns reduce precision (5.6%) but maintain recall (77.8%), GAMs
provide clear explanations for regulators. Combining GAMs with other
models can enhance accuracy while keeping results interpretable for
legal and ethical oversight.

2\. Miller (2025) investigates GAMs for identifying fraudulent financial
statements, often hidden in complex accounting data. GAMs, combined with
models like random forests, detect irregular revenue patterns and
generate interpretable visualizations for auditors. Although effective,
GAMs may miss sophisticated frauds involving multiple interacting
factors. They provide a strong balance of accuracy and clarity for early
detection of financial fraud.

3\. Hanagandi et al. (2023) explore regularized generalized linear
models, including Ridge, Lasso, and ElasticNet, for detecting credit
card fraud in highly imbalanced datasets (0.17% fraud cases). These
models, similar to GAMs, capture complex transaction patterns while
remaining interpretable. Ridge regression achieved high accuracy (up to
98.2%). The study highlights that careful data preparation is crucial
for effective real-time fraud detection in banking environments.

4\. Brossart et al. (2015) discuss the application of GAMs to Medicare
claims data for identifying fraudulent billing and overcharging. GAMs
effectively detect unusual patterns and provide clear visualizations,
which enhance auditor trust. While highly interpretable, they can be
less adaptive to emerging fraud patterns compared to more complex
models, but their transparency makes them valuable for healthcare fraud
investigations.

5\. Graph neural additive networks (GNANs) build on GAMs to detect fraud
in networked data, such as transaction or social networks. GNANs analyze
graph structures using neural networks while keeping results
interpretable with simple visuals. They achieve strong performance
(84.5% ROC-AUC) in tasks like spotting banned or suspicious users. GNANs
help explain fraud in connected datasets and satisfy regulatory
requirements for clear, auditable explanations.

6.Neural additive models (NAMs) extend GAMs using neural networks,
improving their ability to detect fraud in financial datasets like
credit card transactions. NAMs achieve high accuracy (AUC = 0.98) while
remaining interpretable through visualizations. They outperform regular
GAMs in complex scenarios, offering scalability and transparency, which
helps financial institutions comply with regulations and assess bias.

## Methods

Formally, a GAM can be expressed as:

$$ g(\mu) = \alpha + s_1(X_1) + s_2(X_2) + \dots + s_p(X_p) $$

where $g(\mu)$ is the link function (e.g., logit for binary outcomes or
identity for continuous outcomes), $\alpha$ is the intercept, and
$s_j(X_j)$ are smooth functions of the predictor variables $X_j$. This
structure allows each predictor to contribute a smoothed effect to the
model, capturing complex patterns in the data without obscuring the
individual influence of each variable. By balancing flexibility and
clarity, GAMs offer a practical alternative to fully nonparametric
methods, which can become computationally intensive and difficult to
interpret. The additive smooth functions $s_j(X_j)$ are at the heart of
GAMs, enabling the model to uncover nonlinear patterns while maintaining
interpretability for each predictor.

## Analysis and Results

### Data Exploration and Visualization

### Modeling and Results

## Conclusion

## References
